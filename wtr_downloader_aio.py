#!/usr/bin/env python3
"""
wtr_downloader_aio.py
Ø³ÙƒØ±Ø¨Øª Ù„ØªÙ†Ø²ÙŠÙ„ ÙØµÙˆÙ„ Ø±ÙˆØ§ÙŠØ© Ù…Ù† Ù…ÙˆÙ‚Ø¹ WTR-Lab
Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… aiohttp + asyncio + BeautifulSoup
ÙŠØ¯Ø¹Ù…:
 - Ø§ÙƒØªØ´Ø§Ù Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙØµÙˆÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ù…Ù† Ø§Ù„ÙÙ‡Ø±Ø³
 - Ø£Ùˆ Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø§Ù„Ù†Ù…Ø· (chapter-1, chapter-2 ...)
 - ØªÙ†Ø²ÙŠÙ„ Ù…ØªÙˆØ§Ø²ÙŠ Ø¢Ù…Ù† (ÙŠØ­Ø¯ Ø§Ù„ØªÙˆØ§Ø²ÙŠ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±)
 - Ø­ÙØ¸ ÙƒÙ„ ÙØµÙ„ ÙÙŠ Ù…Ù„Ù Ù†ØµÙŠ Ø¯Ø§Ø®Ù„ Ù…Ø¬Ù„Ø¯ chapters/
"""

import argparse
import asyncio
import aiohttp
import aiofiles
import os
import re
import random
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ø§Ù…Ø©
USER_AGENT = "Mozilla/5.0 (compatible; WTR-Downloader/1.0; +https://example.com/bot)"
COMMON_SELECTORS = [
    ".chapter-content", ".entry-content", ".novel-body", ".content", "article",
    "main", "div#content", "div.chapter", "div.chapter-body", "div[itemprop='articleBody']"
]

#------------------------------------------
# Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©
#------------------------------------------

async def fetch_with_retry(session, url, max_retries=5, timeout=25):
    """Ø·Ù„Ø¨ ØµÙØ­Ø© Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø®Ø·Ø£"""
    backoff = 1.0
    for attempt in range(1, max_retries + 1):
        try:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=timeout)) as resp:
                if resp.status == 200:
                    return await resp.text()
                elif resp.status == 404:
                    raise aiohttp.ClientResponseError(resp.request_info, resp.history,
                                                      status=404, message="Not found")
                else:
                    raise aiohttp.ClientError(f"HTTP {resp.status}")
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            if attempt == max_retries:
                raise
            sleep_t = backoff + random.random()
            print(f"âš ï¸  Ø®Ø·Ø£ Ù…Ø¤Ù‚Øª {e}, Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø¹Ø¯ {sleep_t:.1f} Ø«Ø§Ù†ÙŠØ©...")
            await asyncio.sleep(sleep_t)
            backoff *= 2
    raise RuntimeError("ÙØ´Ù„ ÙÙŠ fetch_with_retry Ø¨Ø¹Ø¯ ÙƒÙ„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª")

def extract_chapter_links(html, base_url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙØµÙˆÙ„ Ù…Ù† ØµÙØ­Ø© Ø§Ù„ÙÙ‡Ø±Ø³"""
    soup = BeautifulSoup(html, "lxml")
    anchors = soup.find_all("a", href=True)
    links = []
    patt = re.compile(r'chapter[-_/]?(\d+)', re.I)

    for a in anchors:
        href = a["href"]
        if patt.search(href):
            full = urljoin(base_url, href)
            links.append(full)
        txt = (a.get_text(" ", strip=True) or "").lower()
        if "start reading" in txt:
            full = urljoin(base_url, href)
            links.append(full)

    unique = list(dict.fromkeys(links))
    unique.sort(key=lambda u: int(patt.search(u).group(1)) if patt.search(u) else 0)
    return unique

def guess_chapter_pattern(start_link, base_url):
    """Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù†Ù…Ø· Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙØµÙˆÙ„ Ù…Ø«Ù„ chapter-{}"""
    m = re.search(r'(.*/chapter[-_/]?)(\d+)(/?)$', start_link, re.I)
    if m:
        prefix = m.group(1)
        return urljoin(base_url, prefix + "{}")
    else:
        return base_url.rstrip('/') + "/chapter-{}"

def extract_chapter_content(html):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ù„Ù†Øµ"""
    soup = BeautifulSoup(html, "lxml")
    title_el = soup.find(["h1", "h2"])
    title = title_el.get_text(strip=True) if title_el else "Chapter"
    content = ""
    for sel in COMMON_SELECTORS:
        el = soup.select_one(sel)
        if el and el.get_text(strip=True):
            content = el.get_text("\n\n", strip=True)
            break
    if not content:
        ps = [p.get_text(strip=True) for p in soup.find_all("p") if p.get_text(strip=True)]
        content = "\n\n".join(ps)
    return title, content

#------------------------------------------
# Ø§Ù„ØªÙ†Ø²ÙŠÙ„
#------------------------------------------

async def download_one(session, sem, url, idx, out_dir, delay):
    async with sem:
        try:
            html = await fetch_with_retry(session, url)
            title, content = extract_chapter_content(html)
            safe_title = re.sub(r'[\\/:*?"<>|]', '-', title)[:100]
            filename = os.path.join(out_dir, f"{idx:04d} - {safe_title}.txt")

            if not os.path.exists(out_dir):
                os.makedirs(out_dir)

            # ØªØ®Ø·ÙŠ Ø¥Ù† ÙƒØ§Ù† Ø§Ù„Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹ Ù…Ø³Ø¨Ù‚Ø§Ù‹
            if os.path.exists(filename):
                print(f"[SKIP] Ø§Ù„ÙØµÙ„ {idx} Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø¨Ù‚Ø§Ù‹")
                return

            async with aiofiles.open(filename, "w", encoding="utf-8") as f:
                await f.write(title + "\n\n" + content)
            print(f"[OK] {idx:04d} - {title}")
        except Exception as e:
            async with aiofiles.open(os.path.join(out_dir, "errors.log"), "a", encoding="utf-8") as ef:
                await ef.write(f"{idx:04d} {url} ERROR: {e}\n")
            print(f"[ERR] {idx:04d} ({e})")

        await asyncio.sleep(delay)

#------------------------------------------
# Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
#------------------------------------------

async def main(args):
    url = args.url
    num = args.num
    concurrency = args.concurrency
    delay = args.delay
    out_dir = args.out

    headers = {"User-Agent": USER_AGENT}
    connector = aiohttp.TCPConnector(limit=concurrency * 2)
    sem = asyncio.Semaphore(concurrency)

    async with aiohttp.ClientSession(headers=headers, connector=connector) as session:
        print("ğŸ” ÙŠØ¬Ù„Ø¨ ØµÙØ­Ø© Ø§Ù„ÙÙ‡Ø±Ø³...")
        try:
            html = await fetch_with_retry(session, url)
        except Exception as e:
            print("âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³:", e)
            return

        links = extract_chapter_links(html, url)

        if links:
            print(f"âœ… Ø¹Ø«Ø± Ø¹Ù„Ù‰ {len(links)} Ø±Ø§Ø¨Ø· ÙØµÙ„ØŒ Ø³ÙŠÙØ­Ù…Ù‘Ù„ Ø£ÙˆÙ„ {num}.")
            chapter_links = links[:num]
        else:
            print("âš ï¸ Ù„Ù… ÙŠØ¬Ø¯ Ø±ÙˆØ§Ø¨Ø· ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³ØŒ ÙŠØ­Ø§ÙˆÙ„ Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø§Ù„Ù†Ù…Ø·...")
            start_link = url.rstrip('/') + "/chapter-1"
            pattern = guess_chapter_pattern(start_link, url)
            chapter_links = [pattern.format(i) for i in range(1, num + 1)]
            print(f"ğŸ”— Ø§Ø³ØªÙØ®Ø¯Ù… Ø§Ù„Ù†Ù…Ø·: {pattern}")

        tasks = []
        for idx, link in enumerate(chapter_links, 1):
            tasks.append(download_one(session, sem, link, idx, out_dir, delay))
        await asyncio.gather(*tasks)
        print("\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªÙ†Ø²ÙŠÙ„.")

#------------------------------------------
# ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬
#------------------------------------------

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="WTR-Lab novel downloader (aiohttp).")
    parser.add_argument("url", help="Ø±Ø§Ø¨Ø· ØµÙØ­Ø© Ø§Ù„Ø±ÙˆØ§ÙŠØ© (Ù…Ø«Ù„Ø§Ù‹ https://wtr-lab.com/en/novel/7800/...)")
    parser.add_argument("-n", "--num", type=int, default=5, help="Ø¹Ø¯Ø¯ Ø§Ù„ÙØµÙˆÙ„ Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… ØªÙ†Ø²ÙŠÙ„Ù‡Ø§")
    parser.add_argument("--concurrency", type=int, default=6, help="Ø¹Ø¯Ø¯ Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©")
    parser.add_argument("--delay", type=float, default=0.4, help="ØªØ£Ø®ÙŠØ± Ø¨Ø¹Ø¯ ÙƒÙ„ Ø·Ù„Ø¨ (Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ)")
    parser.add_argument("--out", default="chapters", help="Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬")
    args = parser.parse_args()

    asyncio.run(main(args))
